{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "441018c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber  # Para extraer texto de PDFs\n",
    "import re  # Para trabajar con expresiones regulares\n",
    "import spacy  # Para procesamiento de lenguaje natural (NLP)\n",
    "import unicodedata  # Para normalizar caracteres Unicode\n",
    "import spacy\n",
    "import nltk\n",
    "import stanza\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from huggingface_hub import login\n",
    "from collections import Counter\n",
    "from itertools import islice\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1f35da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud del texto extraído (caracteres): 5026\n",
      "Primeros 600 caracteres del texto:\n",
      "\n",
      "Discurso del Banquete Nobel, 10 de diciembre de 2016  \n",
      "Buenas noches a todos. Extiendo mis más cálidos saludos a los miembros de la Academia \n",
      "Sueca y a todos los demás distinguidos invitados que asisten esta noche.  \n",
      "Lamento no poder estar con ustedes en persona, pero por favor sepan que estoy sin duda con \n",
      "ustedes en espíritu y me siento honrado de recibir un premio tan prestigioso. Ser galardonado \n",
      "con el Premio Nobel de Literatura es algo que jamás podría haber imagin ado o previsto. \n",
      "Desde una edad temprana, he estado familiarizado con, leyendo y absorbiendo las obras de \n",
      "aquellos que fueron considerados dignos de tal distinción: Kipling, Shaw, Thomas Mann, \n",
      "Pearl Buck, Albert Camus, Hemingway. Estos gigantes de la litera tura, cuyas obras se \n",
      "enseñan en las aulas, se guardan en bibliotecas de todo el mundo y se mencionan en tonos \n",
      "reverentes, siempre han dejado en mí una profunda impresión. Que ahora mi nombre se una \n",
      "a esa lista está realmente más allá de las palabras.  \n",
      "No sé si\n"
     ]
    }
   ],
   "source": [
    "# Lectura y análisis de n-gramas del archivo PDF (BobDylan.pdf)\n",
    "# ---------------------------------------------------------------\n",
    "# Este script realiza, paso a paso:\n",
    "# 1) Lectura del PDF (ubicado en /mnt/data/BobDylan.pdf)\n",
    "# 2) Preprocesamiento básico (limpieza y tokenización simple en español)\n",
    "# 3) Extracción y conteo de n-gramas (unigramas, bigramas, trigramas de palabras\n",
    "#    y 3-gramas de caracteres)\n",
    "# 4) Visualizaciones (gráficas separadas y guardadas como PNG)\n",
    "# 5) Modelo simple de bigramas con suavizado Laplace para predecir la siguiente palabra\n",
    "#\n",
    "# Notas:\n",
    "# - Las gráficas usan matplotlib (sin estilos o colores explícitos).\n",
    "# - Intento usar PyPDF2 para extraer texto; si no está disponible, pruebo con fitz (PyMuPDF).\n",
    "# - Cuando sea útil, intento mostrar DataFrames con caas_jupyter_tools.display_dataframe_to_user.\n",
    "# - Los archivos resultantes se guardan en /mnt/data para que puedas descargarlos.\n",
    "#\n",
    "# Ejecuta todo el bloque y revisa las salidas y las imágenes guardadas.\n",
    "\n",
    "# --- 1) Lectura del PDF ---\n",
    "from pathlib import Path\n",
    "pdf_path = Path(\"C:/Users/ACER/OneDrive/Documentos/Analitica de datos/1. Maestria/7. NLP/Clase_4/BobDylan.pdf\")\n",
    "assert pdf_path.exists(), f\"Archivo no encontrado: {pdf_path}\"\n",
    "\n",
    "text = \"\"\n",
    "try:\n",
    "    # Primer intento: PyPDF2\n",
    "    from PyPDF2 import PdfReader\n",
    "    reader = PdfReader(str(pdf_path))\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            text += page_text + \"\\n\"\n",
    "except Exception as e_py:\n",
    "    try:\n",
    "        # Fallback: PyMuPDF (fitz)\n",
    "        import fitz\n",
    "        doc = fitz.open(str(pdf_path))\n",
    "        for page in doc:\n",
    "            text += page.get_text() + \"\\n\"\n",
    "    except Exception as e_fitz:\n",
    "        raise RuntimeError(\"No fue posible extraer texto del PDF con PyPDF2 ni PyMuPDF.\") from e_fitz\n",
    "\n",
    "print(\"Longitud del texto extraído (caracteres):\", len(text))\n",
    "print(\"Primeros 600 caracteres del texto:\\n\")\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5169deb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Número total de tokens (palabras): 817\n",
      "Ejemplo (primeros 40 tokens): ['discurso', 'del', 'banquete', 'nobel', 'de', 'diciembre', 'de', 'buenas', 'noches', 'a', 'todos', 'extiendo', 'mis', 'más', 'cálidos', 'saludos', 'a', 'los', 'miembros', 'de', 'la', 'academia', 'sueca', 'y', 'a', 'todos', 'los', 'demás', 'distinguidos', 'invitados', 'que', 'asisten', 'esta', 'noche', 'lamento', 'no', 'poder', 'estar', 'con', 'ustedes']\n",
      "Número aproximado de oraciones detectadas: 44\n"
     ]
    }
   ],
   "source": [
    "# --- 2) Preprocesamiento y tokenización simple en español ---\n",
    "import re\n",
    "def clean_whitespace(s):\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "raw = clean_whitespace(text)\n",
    "\n",
    "# Tokenizador simple (palabras) para español: conserva letras con tilde y ñ\n",
    "def tokenize_words_spanish(s):\n",
    "    s = s.lower()\n",
    "    # Considera letras a-z y vocales acentuadas + ñ, y permite contracciones simples\n",
    "    tokens = re.findall(r\"[a-záéíóúüñ]+(?:'[a-záéíóúüñ]+)?\", s)\n",
    "    return tokens\n",
    "\n",
    "words = tokenize_words_spanish(raw)\n",
    "print(\"\\nNúmero total de tokens (palabras):\", len(words))\n",
    "print(\"Ejemplo (primeros 40 tokens):\", words[:40])\n",
    "\n",
    "# Frases (sentencias) sencillas usando punto como separador (para algunas visualizaciones)\n",
    "sentences = [clean_whitespace(s) for s in re.split(r\"[.\\n\\r]+\", raw) if s.strip()]\n",
    "print(\"Número aproximado de oraciones detectadas:\", len(sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a42cd65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3) Funciones de n-gramas y conteo ---\n",
    "from collections import Counter\n",
    "\n",
    "def word_ngrams(tokens, n):\n",
    "    if len(tokens) < n:\n",
    "        return []\n",
    "    return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "\n",
    "def char_ngrams(s, n, pad=True):\n",
    "    s2 = re.sub(r\"\\s+\", \" \", s)\n",
    "    if pad:\n",
    "        s2 = \"_\" + s2 + \"_\"\n",
    "    return [s2[i:i+n] for i in range(len(s2)-n+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7e6bbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Unigramas:\n",
      " token  count\n",
      "   de     35\n",
      "  que     33\n",
      "   en     31\n",
      "    y     18\n",
      "   la     16\n",
      "   el     16\n",
      "    a     14\n",
      " para     12\n",
      "  mis      9\n",
      "  más      9\n",
      "\n",
      "Top 10 Bigramas:\n",
      "         ngram  count\n",
      "        en la      6\n",
      "        de la      4\n",
      "        en el      4\n",
      " premio nobel      3\n",
      "     el mundo      3\n",
      "       de que      3\n",
      "mis canciones      3\n",
      "       lo que      3\n",
      "     nobel de      2\n",
      "      a todos      2\n",
      "\n",
      "Top 10 Trigramas:\n",
      "                ngram  count\n",
      "   la academia sueca      2\n",
      "      con ustedes en      2\n",
      "     el premio nobel      2\n",
      "       todo el mundo      2\n",
      "       de que estaba      2\n",
      "     quiénes son los      2\n",
      "para estas canciones      2\n",
      "         en la radio      2\n",
      "     haciendo lo que      2\n",
      "   son mis canciones      2\n",
      "\n",
      "Top 10 3-gramas (caracteres):\n",
      " ngram  count\n",
      "   de     48\n",
      "   es     47\n",
      "  do      43\n",
      "  os      41\n",
      "  est     39\n",
      "  de      38\n",
      "  que     37\n",
      "  en      37\n",
      "   qu     36\n",
      "   en     34\n"
     ]
    }
   ],
   "source": [
    "# Conteos\n",
    "unigrams = Counter(words)\n",
    "bigrams = Counter(word_ngrams(words, 2))\n",
    "trigrams = Counter(word_ngrams(words, 3))\n",
    "char3 = Counter(char_ngrams(raw.lower(), 3))\n",
    "\n",
    "# DataFrames de los top-n para visualización\n",
    "import pandas as pd\n",
    "TOP = 50\n",
    "\n",
    "df_uni = pd.DataFrame(unigrams.most_common(TOP), columns=[\"token\", \"count\"])\n",
    "df_bi = pd.DataFrame([(\" \".join(k), v) for k, v in bigrams.most_common(TOP)], columns=[\"ngram\", \"count\"])\n",
    "df_tri = pd.DataFrame([(\" \".join(k), v) for k, v in trigrams.most_common(TOP)], columns=[\"ngram\", \"count\"])\n",
    "df_char3 = pd.DataFrame(char3.most_common(TOP), columns=[\"ngram\", \"count\"])\n",
    "\n",
    "# Mostrar tablas si caas_jupyter_tools está disponible\n",
    "try:\n",
    "    from caas_jupyter_tools import display_dataframe_to_user\n",
    "    display_dataframe_to_user(\"Top unigrama (palabras)\", df_uni)\n",
    "    display_dataframe_to_user(\"Top bigrama (palabras)\", df_bi)\n",
    "    display_dataframe_to_user(\"Top trigrama (palabras)\", df_tri)\n",
    "    display_dataframe_to_user(\"Top 3-gramas (caracteres)\", df_char3)\n",
    "except Exception:\n",
    "    # Fallback: print parcial\n",
    "    print(\"\\nTop 10 Unigramas:\\n\", df_uni.head(10).to_string(index=False))\n",
    "    print(\"\\nTop 10 Bigramas:\\n\", df_bi.head(10).to_string(index=False))\n",
    "    print(\"\\nTop 10 Trigramas:\\n\", df_tri.head(10).to_string(index=False))\n",
    "    print(\"\\nTop 10 3-gramas (caracteres):\\n\", df_char3.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "732df77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Crear el directorio si no existe\n",
    "os.makedirs('/mnt/data/', exist_ok=True)\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "out_files = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebcf53ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_horizontal_bar' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m out_files \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Modificar las rutas en tu función:\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[43mplot_horizontal_bar\u001b[49m(df_uni, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m Unigramas (palabras)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(TOP), \n\u001b[0;32m     13\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mtop_unigramas.png\u001b[39m\u001b[38;5;124m\"\u001b[39m, top\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m10\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plot_horizontal_bar' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Usar un directorio que seguro existe\n",
    "output_dir = \"./output/\"  # Directorio actual\n",
    "# o\n",
    "output_dir = \"/tmp/\"      # Directorio temporal\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "out_files = []\n",
    "\n",
    "# Modificar las rutas en tu función:\n",
    "plot_horizontal_bar(df_uni, \"token\", \"count\", \"Top {} Unigramas (palabras)\".format(TOP), \n",
    "                   f\"{output_dir}top_unigramas.png\", top=30, figsize=(8,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "052e74c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ejemplos de predicción (bigramas con Laplace):\n",
      "\n",
      "Contexto previo: 'el' -> top predicciones:\n",
      "   mundo  (p≈0.0092)\n",
      "   premio  (p≈0.0069)\n",
      "   honor  (p≈0.0046)\n",
      "   hecho  (p≈0.0046)\n",
      "   año  (p≈0.0046)\n",
      "   gran  (p≈0.0046)\n",
      "   escenario  (p≈0.0046)\n",
      "   financiamiento  (p≈0.0046)\n",
      "\n",
      "Contexto previo: 'la' -> top predicciones:\n",
      "   academia  (p≈0.0069)\n",
      "   radio  (p≈0.0069)\n",
      "   vida  (p≈0.0069)\n",
      "   más  (p≈0.0046)\n",
      "   litera  (p≈0.0046)\n",
      "   luna  (p≈0.0046)\n",
      "   gran  (p≈0.0046)\n",
      "   idea  (p≈0.0046)\n",
      "\n",
      "Contexto previo: 'y' -> top predicciones:\n",
      "   me  (p≈0.0069)\n",
      "   a  (p≈0.0046)\n",
      "   que  (p≈0.0046)\n",
      "   en  (p≈0.0046)\n",
      "   estoy  (p≈0.0046)\n",
      "   absorbiendo  (p≈0.0046)\n",
      "   se  (p≈0.0046)\n",
      "   mujeres  (p≈0.0046)\n",
      "\n",
      "Contexto previo: 'que' -> top predicciones:\n",
      "   estoy  (p≈0.0067)\n",
      "   estaba  (p≈0.0067)\n",
      "   de  (p≈0.0044)\n",
      "   asisten  (p≈0.0044)\n",
      "   me  (p≈0.0044)\n",
      "   el  (p≈0.0044)\n",
      "   es  (p≈0.0044)\n",
      "   jamás  (p≈0.0044)\n",
      "\n",
      "Contexto previo: 'dylan' -> top predicciones:\n",
      "   discurso  (p≈0.0024)\n",
      "   del  (p≈0.0024)\n",
      "   banquete  (p≈0.0024)\n",
      "   nobel  (p≈0.0024)\n",
      "   de  (p≈0.0024)\n",
      "   diciembre  (p≈0.0024)\n",
      "   buenas  (p≈0.0024)\n",
      "   noches  (p≈0.0024)\n",
      "\n",
      "Contexto previo: 'bob' -> top predicciones:\n",
      "   dylan  (p≈0.0048)\n",
      "   discurso  (p≈0.0024)\n",
      "   del  (p≈0.0024)\n",
      "   banquete  (p≈0.0024)\n",
      "   nobel  (p≈0.0024)\n",
      "   de  (p≈0.0024)\n",
      "   diciembre  (p≈0.0024)\n",
      "   buenas  (p≈0.0024)\n",
      "\n",
      "Contexto previo: 'mi' -> top predicciones:\n",
      "   nombre  (p≈0.0048)\n",
      "   mente  (p≈0.0048)\n",
      "   discurso  (p≈0.0024)\n",
      "   del  (p≈0.0024)\n",
      "   banquete  (p≈0.0024)\n",
      "   nobel  (p≈0.0024)\n",
      "   de  (p≈0.0024)\n",
      "   diciembre  (p≈0.0024)\n",
      "\n",
      "Predicciones (ejemplos):\n",
      " prev     next     prob\n",
      "  el    mundo 0.009238\n",
      "  el   premio 0.006928\n",
      "  el    honor 0.004619\n",
      "  el    hecho 0.004619\n",
      "  el      año 0.004619\n",
      "  la academia 0.006928\n",
      "  la    radio 0.006928\n",
      "  la     vida 0.006928\n",
      "  la      más 0.004619\n",
      "  la   litera 0.004619\n",
      "   y       me 0.006897\n",
      "   y        a 0.004598\n",
      "   y      que 0.004598\n",
      "   y       en 0.004598\n",
      "   y    estoy 0.004598\n",
      " que    estoy 0.006667\n",
      " que   estaba 0.006667\n",
      " que       de 0.004444\n",
      " que  asisten 0.004444\n",
      " que       me 0.004444\n",
      "\n",
      "Análisis completado. Si quieres, puedo:\n",
      " - Explicar los gráficos uno por uno en detalle.\n",
      " - Guardar un CSV con las tablas de n-gramas.\n",
      " - Construir un modelo más sofisticado (Kneser-Ney, backoff) o entrenar en segmentos del texto.\n"
     ]
    }
   ],
   "source": [
    "# --- 5) Modelo simple de bigramas con suavizado Laplace (autocompletado) ---\n",
    "V = len(unigrams)\n",
    "alpha = 1.0  # Laplace\n",
    "def prob_bigram(prev, w):\n",
    "    # prev and w are strings (tokens)\n",
    "    return (bigrams.get((prev, w), 0) + alpha) / (unigrams.get(prev, 0) + alpha * V)\n",
    "\n",
    "def predict_next(prev, k=10):\n",
    "    # devuelve los k tokens más probables tras 'prev' con probabilidad estimada\n",
    "    candidates = [(w, prob_bigram(prev, w)) for w in unigrams.keys()]\n",
    "    candidates_sorted = sorted(candidates, key=lambda x: x[1], reverse=True)\n",
    "    return candidates_sorted[:k]\n",
    "\n",
    "# Ejemplos de predicción usando palabras frecuentes en el texto\n",
    "ejemplos_prev = [\"el\", \"la\", \"y\", \"que\", \"dylan\", \"bob\", \"mi\"]\n",
    "print(\"\\nEjemplos de predicción (bigramas con Laplace):\")\n",
    "for p in ejemplos_prev:\n",
    "    preds = predict_next(p, k=8)\n",
    "    # filtrar predicciones no informativas (por ejemplo tokens raros)\n",
    "    print(f\"\\nContexto previo: '{p}' -> top predicciones:\")\n",
    "    for tok, prob in preds[:8]:\n",
    "        print(f\"   {tok}  (p≈{prob:.4f})\")\n",
    "\n",
    "# Guardar una pequeña tabla con ejemplos de predicción\n",
    "preds_table = []\n",
    "for p in ejemplos_prev:\n",
    "    for tok, prob in predict_next(p, k=5):\n",
    "        preds_table.append((p, tok, prob))\n",
    "df_preds = pd.DataFrame(preds_table, columns=[\"prev\", \"next\", \"prob\"])\n",
    "try:\n",
    "    display_dataframe_to_user(\"Predicciones Bigram (ejemplos)\", df_preds)\n",
    "except Exception:\n",
    "    print(\"\\nPredicciones (ejemplos):\\n\", df_preds.head(20).to_string(index=False))\n",
    "\n",
    "# --- Fin del análisis ---\n",
    "print(\"\\nAnálisis completado. Si quieres, puedo:\")\n",
    "print(\" - Explicar los gráficos uno por uno en detalle.\")\n",
    "print(\" - Guardar un CSV con las tablas de n-gramas.\")\n",
    "print(\" - Construir un modelo más sofisticado (Kneser-Ney, backoff) o entrenar en segmentos del texto.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa47576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_files_extra = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b556b29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Nube de palabras (unigramas)\n",
    "wc = WordCloud(width=800, height=500, background_color=\"white\",\n",
    "               colormap=\"viridis\").generate_from_frequencies(unigrams)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.imshow(wc, interpolation=\"bilinear\")\n",
    "ax.axis(\"off\")\n",
    "ax.set_title(\"Nube de palabras (Unigramas más frecuentes)\")\n",
    "plt.tight_layout()\n",
    "fname = \"/mnt/data/wordcloud_unigramas.png\"\n",
    "fig.savefig(fname)\n",
    "plt.show()\n",
    "out_files_extra.append(fname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
